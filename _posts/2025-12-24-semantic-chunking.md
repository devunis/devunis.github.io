---
title: "Semantic Chunking: RAG ì„±ëŠ¥ì„ ì¢Œìš°í•˜ëŠ” ë¬¸ì„œ ë¶„í•  ì „ëµ"
date: 2025-12-24 09:00:00 +0900
categories: [AI, RAG]
tags: [RAG, Chunking, Semantic-Split, Document-Processing]
---

RAG ì‹œìŠ¤í…œì—ì„œ ë¬¸ì„œë¥¼ ì–´ë–»ê²Œ ë‚˜ëˆ„ëŠëƒëŠ” ê²€ìƒ‰ í’ˆì§ˆì— ê²°ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. 
ì´ë²ˆ ê¸€ì—ì„œëŠ” ë‹¤ì–‘í•œ **Chunking ì „ëµ**ê³¼ ê°ê°ì˜ ì¥ë‹¨ì , ê·¸ë¦¬ê³  ìµœì  ì„¤ì •ì„ ì°¾ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

---

## ğŸ¤” Chunkingì´ RAGì— ë¯¸ì¹˜ëŠ” ì˜í–¥

### ë¬¸ì œ ìƒí™©

```python
# ë‚˜ìœ ì˜ˆ: ì „ì²´ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
document = """
(10,000ì ë¶„ëŸ‰ì˜ ê¸´ ê¸°ìˆ  ë¬¸ì„œ)
íŒŒì´ì¬ ì†Œê°œ... asyncio ì„¤ëª…... ì„±ëŠ¥ ìµœì í™”... ë°°í¬ ì „ëµ...
"""

chunks = [document]  # ë‹¨ 1ê°œì˜ ì²­í¬

# ë¬¸ì œì :
# 1. ì„ë² ë”©ì´ ë„ˆë¬´ ì¼ë°˜ì  â†’ ê²€ìƒ‰ ì •í™•ë„ í•˜ë½
# 2. LLM ì»¨í…ìŠ¤íŠ¸ í•œê³„ ì´ˆê³¼ ê°€ëŠ¥
# 3. ê´€ë ¨ ì—†ëŠ” ì •ë³´ê¹Œì§€ í¬í•¨ â†’ ë…¸ì´ì¦ˆ ì¦ê°€
```

### ì¢‹ì€ ì˜ˆ: ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• 

```python
chunks = [
    "íŒŒì´ì¬ ì†Œê°œ: íŒŒì´ì¬ì€...",           # 200ì
    "asyncio ê°œë…: ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°...",  # 250ì
    "ì„±ëŠ¥ ìµœì í™”: ë‹¤ìŒ ë°©ë²•ë“¤ë¡œ...",       # 180ì
]

# ì¥ì :
# 1. ê° ì²­í¬ê°€ ëª…í™•í•œ ì£¼ì œ
# 2. ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ
# 3. LLM ì»¨í…ìŠ¤íŠ¸ íš¨ìœ¨ì  ì‚¬ìš©
```

---

## ğŸ“ Fixed-size vs Semantic Chunking

### 1. Fixed-size Chunking

**ê³ ì •ëœ ë¬¸ì/í† í° ìˆ˜ë¡œ ë¶„í• **

```python
def fixed_size_chunking(text: str, chunk_size: int = 500, overlap: int = 50):
    """ê³ ì • í¬ê¸° ì²­í‚¹"""
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start = end - overlap  # ì˜¤ë²„ë© ì ìš©
    
    return chunks

# ì‚¬ìš©
text = "ê¸´ ë¬¸ì„œ ë‚´ìš©..."
chunks = fixed_size_chunking(text, chunk_size=500, overlap=50)
```

**ì¥ì **:
- âœ… êµ¬í˜„ ê°„ë‹¨
- âœ… ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì²­í¬ ìˆ˜
- âœ… ë¹ ë¥¸ ì²˜ë¦¬

**ë‹¨ì **:
- âŒ ë¬¸ì¥ ì¤‘ê°„ì—ì„œ ì˜ë¦¼
- âŒ ë¬¸ë§¥ ì†ì‹¤ ê°€ëŠ¥
- âŒ ì˜ë¯¸ ë‹¨ìœ„ ë¬´ì‹œ

### 2. Semantic Chunking

**ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• **

```python
from langchain.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings

# ì„ë² ë”© ê¸°ë°˜ ì˜ë¯¸ ë¶„í• 
text_splitter = SemanticChunker(
    OpenAIEmbeddings(),
    breakpoint_threshold_type="percentile"  # ì˜ë¯¸ ë³€í™” ê°ì§€
)

chunks = text_splitter.split_text(text)

# ê° ì²­í¬ëŠ” ì˜ë¯¸ì ìœ¼ë¡œ ì—°ê´€ëœ ë‚´ìš©ë§Œ í¬í•¨
```

**ì¥ì **:
- âœ… ë¬¸ë§¥ ë³´ì¡´
- âœ… ì˜ë¯¸ ë‹¨ìœ„ ìœ ì§€
- âœ… ë” ë‚˜ì€ ê²€ìƒ‰ í’ˆì§ˆ

**ë‹¨ì **:
- âŒ ëŠë¦° ì²˜ë¦¬ (ì„ë² ë”© í•„ìš”)
- âŒ ì²­í¬ í¬ê¸° ë¶ˆê· ë“±
- âŒ ë³µì¡í•œ êµ¬í˜„

---

## ğŸªŸ Sliding Window ê¸°ë²•

ì²­í¬ ê°„ ì˜¤ë²„ë©ì„ í†µí•´ ë¬¸ë§¥ ì—°ê²°ì„± ìœ ì§€!

```python
text = "A B C D E F G H I J"

# ì˜¤ë²„ë© ì—†ìŒ
chunks = ["A B C", "D E F", "G H I"]  # ë¬¸ë§¥ ë‹¨ì ˆ

# ì˜¤ë²„ë© ìˆìŒ (1ê°œ)
chunks = ["A B C", "C D E", "E F G", "G H I"]  # ì—°ê²°ì„± ìœ ì§€
```

### êµ¬í˜„

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=100,  # 20% ì˜¤ë²„ë©
    length_function=len,
    separators=["\n\n", "\n", ". ", " ", ""]
)

chunks = splitter.split_text(text)

# ì˜ˆì‹œ ì¶œë ¥
print(f"ì²­í¬ 1: ...ì„±ëŠ¥ ìµœì í™”ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.")
print(f"ì²­í¬ 2: ì„±ëŠ¥ ìµœì í™”ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤. ë‹¤ìŒ ë°©ë²•ì„...")
#                â†‘ ì˜¤ë²„ë© ì˜ì—­
```

**ì˜¤ë²„ë© í¬ê¸° ì„ íƒ ê°€ì´ë“œ**:
- ğŸ“Œ 10-20%: ì¼ë°˜ì ì¸ ê²½ìš°
- ğŸ“Œ 20-30%: ë†’ì€ ë¬¸ë§¥ ì˜ì¡´ì„±
- ğŸ“Œ 5-10%: ë©”ëª¨ë¦¬ ì œì•½

---

## ğŸ“„ ë¬¸ì¥/ë‹¨ë½ ê¸°ë°˜ ë¶„í• 

### ë¬¸ì¥ ê¸°ë°˜

```python
from langchain.text_splitter import CharacterTextSplitter

# ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
splitter = CharacterTextSplitter(
    separator=". ",
    chunk_size=1000,
    chunk_overlap=0
)

chunks = splitter.split_text(text)
```

### ë‹¨ë½ ê¸°ë°˜

```python
# ë‹¨ë½(ì´ì¤‘ ì¤„ë°”ê¿ˆ) ë‹¨ìœ„
splitter = CharacterTextSplitter(
    separator="\n\n",
    chunk_size=1000,
    chunk_overlap=100
)

chunks = splitter.split_text(text)
```

### ê³„ì¸µì  ë¶„í• 

```python
# ìš°ì„ ìˆœìœ„: ë‹¨ë½ > ë¬¸ì¥ > ë‹¨ì–´
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=[
        "\n\n",  # ë‹¨ë½
        "\n",    # ì¤„ë°”ê¿ˆ
        ". ",    # ë¬¸ì¥
        " ",     # ë‹¨ì–´
        ""       # ë¬¸ì
    ]
)

# ê°€ëŠ¥í•œ í•œ í° ë‹¨ìœ„ë¡œ ë¶„í•  ì‹œë„
chunks = splitter.split_text(text)
```

---

## ğŸ”§ LangChain RecursiveCharacterTextSplitter ì‹¤ì „

### ê¸°ë³¸ ì‚¬ìš©

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    is_separator_regex=False,
)

# ë¬¸ì„œ ë¶„í• 
texts = text_splitter.create_documents([long_document])

# ë©”íƒ€ë°ì´í„° í¬í•¨
texts = text_splitter.create_documents(
    [long_document],
    metadatas=[{"source": "doc1.pdf", "page": 1}]
)
```

### ì½”ë“œ íŠ¹í™” ë¶„í• 

```python
from langchain.text_splitter import Language, RecursiveCharacterTextSplitter

# Python ì½”ë“œ ë¶„í• 
python_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PYTHON,
    chunk_size=500,
    chunk_overlap=50
)

code = """
def example():
    # í•¨ìˆ˜ ì •ì˜
    pass

class MyClass:
    # í´ë˜ìŠ¤ ì •ì˜
    pass
"""

chunks = python_splitter.split_text(code)
# í•¨ìˆ˜/í´ë˜ìŠ¤ ë‹¨ìœ„ë¡œ ë¶„í•  ì‹œë„
```

### Markdown íŠ¹í™”

```python
from langchain.text_splitter import MarkdownHeaderTextSplitter

# í—¤ë” ê¸°ë°˜ ë¶„í• 
headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
]

markdown_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=headers_to_split_on
)

markdown_text = """
# Introduction
This is intro.

## Section 1
Content 1.

## Section 2
Content 2.
"""

chunks = markdown_splitter.split_text(markdown_text)

# ê° ì²­í¬ëŠ” í—¤ë” ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ í¬í•¨
for chunk in chunks:
    print(chunk.metadata)  # {"Header 1": "Introduction", "Header 2": "Section 1"}
```

---

## ğŸ¯ ìµœì  Chunk í¬ê¸° ê²°ì • ë°©ë²•

### ì‹¤í—˜ì  ì ‘ê·¼

```python
import numpy as np
from sklearn.metrics import ndcg_score

def evaluate_chunk_size(documents, queries, chunk_size, overlap):
    """ì²­í¬ í¬ê¸°ë³„ ì„±ëŠ¥ í‰ê°€"""
    
    # 1. ë¬¸ì„œ ë¶„í• 
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=overlap
    )
    chunks = splitter.split_documents(documents)
    
    # 2. ë²¡í„° DB êµ¬ì¶•
    vectorstore = build_vectorstore(chunks)
    
    # 3. ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€
    scores = []
    for query, relevant_docs in queries:
        results = vectorstore.similarity_search(query, k=5)
        score = calculate_relevance(results, relevant_docs)
        scores.append(score)
    
    return np.mean(scores)

# ë‹¤ì–‘í•œ í¬ê¸° ì‹¤í—˜
results = {}
for size in [200, 500, 1000, 1500, 2000]:
    for overlap in [0, 50, 100, 200]:
        score = evaluate_chunk_size(docs, queries, size, overlap)
        results[(size, overlap)] = score

# ìµœì  ì„¤ì • ì°¾ê¸°
best_config = max(results, key=results.get)
print(f"ìµœì  ì„¤ì •: chunk_size={best_config[0]}, overlap={best_config[1]}")
```

### ì‹¤í—˜ ê²°ê³¼ (SenPick ì‚¬ë¡€)

| Chunk Size | Overlap | NDCG@5 | ì‘ë‹µ ì‹œê°„ |
|-----------|---------|---------|----------|
| 200 | 0 | 0.65 | 1.2s |
| 500 | 50 | 0.78 | 1.5s |
| **1000** | **100** | **0.85** | **1.8s** |
| 1500 | 150 | 0.83 | 2.1s |
| 2000 | 200 | 0.79 | 2.5s |

**ê²°ë¡ **: chunk_size=1000, overlap=100ì´ ìµœì !

---

## ğŸ” Overlap ì „ëµ ë° ì‹¤í—˜

### Overlapì´ ì¤‘ìš”í•œ ì´ìœ 

```python
# ì˜¤ë²„ë© ì—†ìŒ
chunk1 = "...íŒŒì´ì¬ì€ ê°•ë ¥í•œ ì–¸ì–´ì…ë‹ˆë‹¤."
chunk2 = "ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì€..."
# ë¬¸ì œ: "íŒŒì´ì¬ì˜ ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°" ì¿¼ë¦¬ì— ì•½í•¨

# ì˜¤ë²„ë© ìˆìŒ
chunk1 = "...íŒŒì´ì¬ì€ ê°•ë ¥í•œ ì–¸ì–´ì…ë‹ˆë‹¤. ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì€..."
chunk2 = "ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì€... asyncioë¥¼ ì‚¬ìš©í•˜ë©´..."
# í•´ê²°: ë¬¸ë§¥ ì—°ê²° ìœ ì§€
```

### ë™ì  ì˜¤ë²„ë©

```python
def dynamic_overlap_chunking(text, base_chunk_size=1000):
    """ë¬¸ë§¥ì— ë”°ë¼ ì˜¤ë²„ë© ì¡°ì •"""
    
    chunks = []
    sentences = text.split('. ')
    
    current_chunk = []
    current_size = 0
    
    for sentence in sentences:
        sentence_len = len(sentence)
        
        if current_size + sentence_len > base_chunk_size:
            # ì²­í¬ ì™„ì„±
            chunk_text = '. '.join(current_chunk)
            chunks.append(chunk_text)
            
            # ì˜¤ë²„ë©: ë§ˆì§€ë§‰ 2-3ë¬¸ì¥ ìœ ì§€
            overlap_sentences = current_chunk[-2:]
            current_chunk = overlap_sentences + [sentence]
            current_size = sum(len(s) for s in current_chunk)
        else:
            current_chunk.append(sentence)
            current_size += sentence_len
    
    # ë§ˆì§€ë§‰ ì²­í¬
    if current_chunk:
        chunks.append('. '.join(current_chunk))
    
    return chunks
```

---

## ğŸ¨ ê³ ê¸‰ ì „ëµ

### 1. Hierarchical Chunking

```python
# ê³„ì¸µì  ì²­í‚¹: í° ì²­í¬ + ì‘ì€ ì²­í¬
def hierarchical_chunking(text):
    # ë ˆë²¨ 1: í° ì²­í¬ (ë¬¸ë§¥ íŒŒì•…)
    large_splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000,
        chunk_overlap=200
    )
    large_chunks = large_splitter.split_text(text)
    
    # ë ˆë²¨ 2: ì‘ì€ ì²­í¬ (ì •ë°€ ê²€ìƒ‰)
    small_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50
    )
    
    result = []
    for large_chunk in large_chunks:
        small_chunks = small_splitter.split_text(large_chunk)
        for small_chunk in small_chunks:
            result.append({
                "small_chunk": small_chunk,
                "large_chunk": large_chunk,  # ë¬¸ë§¥ ì •ë³´
            })
    
    return result
```

### 2. Context-Enriched Chunking

```python
def context_enriched_chunking(document):
    """ì²­í¬ì— ë¬¸ë§¥ ì •ë³´ ì¶”ê°€"""
    
    # ê¸°ë³¸ ë¶„í• 
    chunks = split_document(document)
    
    enriched_chunks = []
    for i, chunk in enumerate(chunks):
        # ë¬¸ì„œ ì œëª©, ì„¹ì…˜ ì •ë³´ ì¶”ê°€
        enriched = f"""
        [Document: {document.title}]
        [Section: {document.section}]
        [Chunk {i+1}/{len(chunks)}]
        
        {chunk}
        """
        enriched_chunks.append(enriched)
    
    return enriched_chunks
```

---

## ğŸ“Š ì‹¤ì „ ì„±ëŠ¥ ë¹„êµ

### í…ŒìŠ¤íŠ¸ í™˜ê²½
- ë¬¸ì„œ: 50ê°œ ê¸°ìˆ  ë¸”ë¡œê·¸ (í‰ê·  5,000ì)
- ì¿¼ë¦¬: 100ê°œ ì§ˆë¬¸
- í‰ê°€ ì§€í‘œ: NDCG@5, MRR

### ê²°ê³¼

| ì „ëµ | NDCG@5 | MRR | ì²­í¬ ìˆ˜ |
|------|--------|-----|---------|
| Fixed(500) | 0.68 | 0.72 | 500 |
| Fixed(1000) | 0.75 | 0.79 | 250 |
| Semantic | 0.82 | 0.86 | 320 |
| **Recursive(1000, 100)** | **0.85** | **0.89** | 280 |
| Hierarchical | 0.84 | 0.88 | 450 |

---

## ğŸ¯ ì •ë¦¬ ë° ê¶Œì¥ì‚¬í•­

### ê¸°ë³¸ ê¶Œì¥ ì„¤ì •

```python
# ë²”ìš© ì„¤ì •
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=100,  # 10%
    separators=["\n\n", "\n", ". ", " ", ""]
)
```

### ìƒí™©ë³„ ì„ íƒ ê°€ì´ë“œ

| ìƒí™© | ì¶”ì²œ ì „ëµ |
|------|----------|
| ì¼ë°˜ ë¬¸ì„œ | Recursive (1000, 100) |
| ì½”ë“œ | Language-specific |
| Markdown | Header-based |
| ì§§ì€ ë¬¸ì„œ | Fixed (500, 50) |
| ê¸´ ë¬¸ì„œ | Hierarchical |

### ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ì²­í¬ í¬ê¸°ëŠ” ì„ë² ë”© ëª¨ë¸ í•œê³„ ê³ ë ¤
- [ ] ì˜¤ë²„ë©ì€ 10-20% ê¶Œì¥
- [ ] ë¬¸ì¥ ì¤‘ê°„ ë¶„í•  ë°©ì§€
- [ ] ë©”íƒ€ë°ì´í„° í¬í•¨
- [ ] ì„±ëŠ¥ ì‹¤í—˜ìœ¼ë¡œ ìµœì í™”

Chunkingì€ RAG ì„±ëŠ¥ì˜ ê¸°ì´ˆì…ë‹ˆë‹¤. ì ì ˆí•œ ì „ëµì„ ì„ íƒí•˜ê³  ì‹¤í—˜ì„ í†µí•´ ìµœì í™”í•˜ë©´ 
ê²€ìƒ‰ í’ˆì§ˆì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ¯

---

ğŸ“š **ì°¸ê³  ìë£Œ**:
- [LangChain Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)
