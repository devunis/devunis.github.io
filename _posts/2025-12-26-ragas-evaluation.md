---
title: "RAG ì‹œìŠ¤í…œ í‰ê°€ ì§€í‘œ: RAGASë¡œ í’ˆì§ˆ ì¸¡ì •í•˜ê¸°"
date: 2025-12-26 09:00:00 +0900
categories: [AI, RAG]
tags: [RAG, RAGAS, Evaluation, Metrics, Quality]
---

"ìš°ë¦¬ RAG ì‹œìŠ¤í…œì´ ì˜ ì‘ë™í•˜ê³  ìˆë‚˜ìš”?" - ì´ ì§ˆë¬¸ì— ê°ê´€ì ìœ¼ë¡œ ë‹µí•˜ê¸° ìœ„í•´ **RAGAS** í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•œ 
RAG í‰ê°€ ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.

---

## ğŸ¯ RAG í‰ê°€ì˜ ì¤‘ìš”ì„±

### ì™œ í‰ê°€ê°€ í•„ìš”í•œê°€?

```python
# ì£¼ê´€ì  í‰ê°€ âŒ
"ìŒ... ë‹µë³€ì´ ê´œì°®ì€ ê²ƒ ê°™ì€ë°?"

# ê°ê´€ì  í‰ê°€ âœ…
"Faithfulness: 0.87, Relevancy: 0.92"
```

**í‰ê°€ ì—†ì´ëŠ”**:
- âŒ ê°œì„  ë°©í–¥ ë¶ˆëª…í™•
- âŒ A/B í…ŒìŠ¤íŠ¸ ë¶ˆê°€
- âŒ ì„±ëŠ¥ í‡´í™” ê°ì§€ ë¶ˆê°€
- âŒ íŒ€ ê°„ ì†Œí†µ ì–´ë ¤ì›€

---

## ğŸ“Š ì£¼ìš” í‰ê°€ ì§€í‘œ

### 1. Faithfulness (ì¶©ì‹¤ì„±)

**ì •ì˜**: ìƒì„±ëœ ë‹µë³€ì´ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°í•˜ëŠ”ê°€?

```python
# High Faithfulness âœ…
Context: "FastAPIëŠ” Python 3.6+ì˜ ë¹„ë™ê¸° ì›¹ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."
Answer: "FastAPIëŠ” Python 3.6 ì´ìƒì—ì„œ ë™ì‘í•˜ëŠ” ë¹„ë™ê¸° í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."
# â†’ ì»¨í…ìŠ¤íŠ¸ì— ì¶©ì‹¤

# Low Faithfulness âŒ
Answer: "FastAPIëŠ” 2015ë…„ì— ì¶œì‹œëœ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."
# â†’ ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ (í™˜ê°)
```

**ê³„ì‚° ë°©ë²•**:
```
Faithfulness = (ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê²€ì¦ ê°€ëŠ¥í•œ ë¬¸ì¥ ìˆ˜) / (ì „ì²´ ë¬¸ì¥ ìˆ˜)
```

### 2. Answer Relevancy (ë‹µë³€ ê´€ë ¨ì„±)

**ì •ì˜**: ë‹µë³€ì´ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ëŠ”ê°€?

```python
Question: "FastAPIì˜ ì„±ëŠ¥ ìµœì í™” ë°©ë²•ì€?"

# High Relevancy âœ…
Answer: "FastAPI ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ 1) ë¹„ë™ê¸° ì²˜ë¦¬ í™œìš©, 2) Pydantic ëª¨ë¸ ìºì‹±..."

# Low Relevancy âŒ
Answer: "FastAPIëŠ” Python ì›¹ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. Djangoì™€ ë‹¤ë¥´ê²Œ..."
# â†’ ì§ˆë¬¸ê³¼ ë¬´ê´€í•œ ë‚´ìš©
```

### 3. Context Recall (ì»¨í…ìŠ¤íŠ¸ ì¬í˜„ìœ¨)

**ì •ì˜**: ì •ë‹µì— í•„ìš”í•œ ì •ë³´ê°€ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?

```python
Ground Truth: "FastAPIëŠ” Starletteì™€ Pydanticì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤."

Retrieved Context:
- Doc1: "FastAPIëŠ” Starletteë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."  âœ…
- Doc2: "FastAPIëŠ” ë¹ ë¥¸ ì„±ëŠ¥ì„ ìë‘í•©ë‹ˆë‹¤."  âš ï¸
# Pydantic ì •ë³´ ëˆ„ë½ â†’ Recall ë‚®ìŒ
```

### 4. Context Precision (ì»¨í…ìŠ¤íŠ¸ ì •ë°€ë„)

**ì •ì˜**: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ë†’ì€ê°€?

```python
Query: "FastAPI ë¹„ë™ê¸° ì²˜ë¦¬"

# High Precision âœ…
Retrieved:
1. "FastAPIì˜ async/await ì‚¬ìš©ë²•"
2. "ë¹„ë™ê¸° ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™"

# Low Precision âŒ
Retrieved:
1. "FastAPI ì„¤ì¹˜ ë°©ë²•"
2. "FastAPI ì—­ì‚¬"
3. "Djangoì™€ì˜ ë¹„êµ"
4. "FastAPI ë¹„ë™ê¸° ì²˜ë¦¬"  â† ê´€ë ¨ ë¬¸ì„œëŠ” 1ê°œë¿
```

---

## ğŸ”§ RAGAS í”„ë ˆì„ì›Œí¬ ì†Œê°œ

**RAGAS** (RAG Assessment)ëŠ” LLMì„ í™œìš©í•œ RAG í‰ê°€ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

### ì„¤ì¹˜

```bash
pip install ragas
```

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
)
from datasets import Dataset

# í‰ê°€ ë°ì´í„° ì¤€ë¹„
data = {
    "question": ["FastAPIì˜ ì¥ì ì€?", "Django vs FastAPI"],
    "answer": [
        "FastAPIëŠ” ë¹ ë¥´ê³  ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.",
        "FastAPIëŠ” ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ê³ , DjangoëŠ” ê¸°ëŠ¥ì´ í’ë¶€í•©ë‹ˆë‹¤."
    ],
    "contexts": [
        ["FastAPIëŠ” ë†’ì€ ì„±ëŠ¥ì˜ ë¹„ë™ê¸° ì›¹ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."],
        ["FastAPIëŠ” ë¹ ë¥¸ ì„±ëŠ¥, DjangoëŠ” ì™„ì„±ë„ ë†’ì€ ê¸°ëŠ¥ ì œê³µ"]
    ],
    "ground_truths": [
        "FastAPIëŠ” ë¹ ë¥¸ ì„±ëŠ¥ê³¼ ë¹„ë™ê¸° ì²˜ë¦¬ê°€ ì¥ì ì…ë‹ˆë‹¤.",
        "FastAPIëŠ” ì„±ëŠ¥, DjangoëŠ” ìƒíƒœê³„ê°€ ê°•ì ì…ë‹ˆë‹¤."
    ]
}

dataset = Dataset.from_dict(data)

# í‰ê°€ ì‹¤í–‰
result = evaluate(
    dataset,
    metrics=[
        faithfulness,
        answer_relevancy,
        context_recall,
        context_precision,
    ],
)

print(result)
```

**ì¶œë ¥**:
```python
{
    'faithfulness': 0.87,
    'answer_relevancy': 0.92,
    'context_recall': 0.85,
    'context_precision': 0.89
}
```

---

## ğŸ’» ì‹¤ì „ RAG ì‹œìŠ¤í…œ í‰ê°€

### 1. ë°ì´í„° ìˆ˜ì§‘

```python
from langchain.vectorstores import Qdrant
from langchain.chains import RetrievalQA

# RAG ì‹œìŠ¤í…œ ì‹¤í–‰
def evaluate_rag_system(questions, ground_truths):
    """RAG ì‹œìŠ¤í…œ í‰ê°€ ë°ì´í„° ìƒì„±"""
    
    evaluation_data = {
        "question": [],
        "answer": [],
        "contexts": [],
        "ground_truths": []
    }
    
    for question, truth in zip(questions, ground_truths):
        # 1. ê²€ìƒ‰
        docs = retriever.get_relevant_documents(question)
        contexts = [doc.page_content for doc in docs]
        
        # 2. ë‹µë³€ ìƒì„±
        answer = qa_chain({"query": question})["result"]
        
        # 3. ë°ì´í„° ì €ì¥
        evaluation_data["question"].append(question)
        evaluation_data["answer"].append(answer)
        evaluation_data["contexts"].append(contexts)
        evaluation_data["ground_truths"].append(truth)
    
    return evaluation_data

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
test_questions = [
    "Python asyncioë€?",
    "FastAPI ì„¤ì¹˜ ë°©ë²•ì€?",
    # ... ë” ë§ì€ ì§ˆë¬¸
]

test_ground_truths = [
    "asyncioëŠ” Pythonì˜ ë¹„ë™ê¸° I/O ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.",
    "pip install fastapië¡œ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    # ... ì •ë‹µ
]

# í‰ê°€ ë°ì´í„° ìƒì„±
eval_data = evaluate_rag_system(test_questions, test_ground_truths)
```

### 2. í‰ê°€ ì‹¤í–‰

```python
from ragas import evaluate
from datasets import Dataset

dataset = Dataset.from_dict(eval_data)

# í‰ê°€
results = evaluate(
    dataset,
    metrics=[faithfulness, answer_relevancy, context_recall, context_precision]
)

print(f"Faithfulness: {results['faithfulness']:.2f}")
print(f"Answer Relevancy: {results['answer_relevancy']:.2f}")
print(f"Context Recall: {results['context_recall']:.2f}")
print(f"Context Precision: {results['context_precision']:.2f}")
```

### 3. ìƒì„¸ ë¶„ì„

```python
# ì§ˆë¬¸ë³„ ì ìˆ˜ í™•ì¸
for i, question in enumerate(eval_data["question"]):
    print(f"\nì§ˆë¬¸: {question}")
    print(f"  Faithfulness: {results.scores[i]['faithfulness']:.2f}")
    print(f"  Relevancy: {results.scores[i]['answer_relevancy']:.2f}")
    
    # ë‚®ì€ ì ìˆ˜ í•­ëª© ë””ë²„ê¹…
    if results.scores[i]['faithfulness'] < 0.7:
        print(f"  âš ï¸ ë‚®ì€ ì¶©ì‹¤ì„± - í™˜ê° ê°€ëŠ¥ì„±")
        print(f"  ë‹µë³€: {eval_data['answer'][i]}")
        print(f"  ì»¨í…ìŠ¤íŠ¸: {eval_data['contexts'][i][0][:100]}...")
```

---

## ğŸ“ˆ í‰ê°€ ê²°ê³¼ í•´ì„

### ì ìˆ˜ ê¸°ì¤€

| ì§€í‘œ | ìš°ìˆ˜ | ì–‘í˜¸ | ê°œì„  í•„ìš” |
|------|------|------|----------|
| Faithfulness | >0.9 | 0.7-0.9 | <0.7 |
| Answer Relevancy | >0.85 | 0.7-0.85 | <0.7 |
| Context Recall | >0.8 | 0.6-0.8 | <0.6 |
| Context Precision | >0.8 | 0.6-0.8 | <0.6 |

### SenPick ì‹¤ì œ ê²°ê³¼

#### Before (ê¸°ë³¸ RAG)
```python
{
    'faithfulness': 0.72,        # í™˜ê° ë§ìŒ
    'answer_relevancy': 0.78,    # ì§ˆë¬¸ ì´íƒˆ
    'context_recall': 0.65,      # ì •ë³´ ëˆ„ë½
    'context_precision': 0.70    # ë…¸ì´ì¦ˆ ë§ìŒ
}
```

#### After (Reranker + Chunking ìµœì í™”)
```python
{
    'faithfulness': 0.89,        # â†‘ +17%
    'answer_relevancy': 0.91,    # â†‘ +13%
    'context_recall': 0.83,      # â†‘ +18%
    'context_precision': 0.87    # â†‘ +17%
}
```

---

## ğŸ”§ ê°œì„  ì „ëµ ìˆ˜ë¦½

### 1. Faithfulness ê°œì„ 

**ë¬¸ì œ**: ë‹µë³€ì— í™˜ê°(Hallucination) ë°œìƒ

**í•´ê²°ì±…**:
```python
# 1. í”„ë¡¬í”„íŠ¸ ê°œì„ 
prompt = """
ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ëŠ” "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸: {context}
ì§ˆë¬¸: {question}
"""

# 2. Temperature ë‚®ì¶”ê¸°
llm = ChatOpenAI(temperature=0.0)  # ë” ê²°ì •ë¡ ì 

# 3. ì»¨í…ìŠ¤íŠ¸ ê°•ì¡°
prompt = """
âš ï¸ ì¤‘ìš”: ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.

{context}
"""
```

### 2. Context Recall ê°œì„ 

**ë¬¸ì œ**: ì¤‘ìš” ì •ë³´ê°€ ê²€ìƒ‰ë˜ì§€ ì•ŠìŒ

**í•´ê²°ì±…**:
```python
# 1. ê²€ìƒ‰ ê°œìˆ˜ ì¦ê°€
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 10}  # 5 â†’ 10
)

# 2. Reranker ë„ì…
from langchain.retrievers import ContextualCompressionRetriever

compression_retriever = ContextualCompressionRetriever(
    base_retriever=retriever,
    base_compressor=reranker
)

# 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
from langchain.retrievers import EnsembleRetriever

ensemble = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.5, 0.5]
)
```

### 3. Context Precision ê°œì„ 

**ë¬¸ì œ**: ë¬´ê´€í•œ ë¬¸ì„œê°€ ë§ì´ ê²€ìƒ‰ë¨

**í•´ê²°ì±…**:
```python
# 1. ìœ ì‚¬ë„ ì„ê³„ê°’ ì„¤ì •
docs = vectorstore.similarity_search_with_score(query, k=10)
filtered_docs = [doc for doc, score in docs if score > 0.7]

# 2. Metadata í•„í„°ë§
retriever = vectorstore.as_retriever(
    search_kwargs={
        "k": 5,
        "filter": {"category": "backend"}
    }
)

# 3. Rerankerë¡œ ìƒìœ„ kê°œë§Œ
final_docs = reranker.rerank(docs, top_n=3)
```

---

## ğŸ”¬ A/B í…ŒìŠ¤íŠ¸ ë°©ë²•

```python
def compare_rag_versions(version_a, version_b, test_data):
    """ë‘ RAG ì‹œìŠ¤í…œ ë¹„êµ"""
    
    # Version A í‰ê°€
    results_a = evaluate_rag_system(version_a, test_data)
    scores_a = evaluate(Dataset.from_dict(results_a), metrics=[...])
    
    # Version B í‰ê°€
    results_b = evaluate_rag_system(version_b, test_data)
    scores_b = evaluate(Dataset.from_dict(results_b), metrics=[...])
    
    # ë¹„êµ
    print("=== A/B Test Results ===")
    for metric in ['faithfulness', 'answer_relevancy', 'context_recall']:
        diff = scores_b[metric] - scores_a[metric]
        symbol = "â†‘" if diff > 0 else "â†“"
        print(f"{metric}: {scores_a[metric]:.2f} â†’ {scores_b[metric]:.2f} ({symbol}{abs(diff):.2f})")
    
    # í†µê³„ì  ìœ ì˜ì„± ê²€ì •
    from scipy.stats import ttest_rel
    t_stat, p_value = ttest_rel(scores_a.scores, scores_b.scores)
    print(f"\np-value: {p_value:.4f}")
    print("í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸" if p_value < 0.05 else "ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ")

# ì‚¬ìš© ì˜ˆì‹œ
compare_rag_versions(
    version_a={"retriever": basic_retriever, "llm": gpt35},
    version_b={"retriever": reranker_retriever, "llm": gpt4},
    test_data=test_questions
)
```

---

## ğŸ“Š ì‹œê°í™” ë° ëª¨ë‹ˆí„°ë§

```python
import matplotlib.pyplot as plt
import pandas as pd

def visualize_evaluation(results_over_time):
    """í‰ê°€ ê²°ê³¼ ì‹œê°í™”"""
    
    df = pd.DataFrame(results_over_time)
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    metrics = ['faithfulness', 'answer_relevancy', 
               'context_recall', 'context_precision']
    
    for i, (ax, metric) in enumerate(zip(axes.flat, metrics)):
        ax.plot(df['date'], df[metric], marker='o')
        ax.set_title(metric.replace('_', ' ').title())
        ax.set_xlabel('Date')
        ax.set_ylabel('Score')
        ax.grid(True, alpha=0.3)
        ax.set_ylim(0, 1)
    
    plt.tight_layout()
    plt.savefig('rag_evaluation_trends.png')
    plt.show()

# ì‚¬ìš©
results_history = [
    {'date': '2025-01-01', 'faithfulness': 0.75, ...},
    {'date': '2025-01-08', 'faithfulness': 0.82, ...},
    # ...
]

visualize_evaluation(results_history)
```

---

## ğŸ¯ ëª¨ë‹ˆí„°ë§ ìë™í™”

```python
from datetime import datetime
import json

class RAGMonitor:
    """í”„ë¡œë•ì…˜ RAG ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, rag_system, alert_threshold=0.7):
        self.rag_system = rag_system
        self.threshold = alert_threshold
        self.history = []
    
    def daily_evaluation(self, test_questions, ground_truths):
        """ì¼ì¼ í‰ê°€ ì‹¤í–‰"""
        
        # í‰ê°€ ë°ì´í„° ìƒì„±
        eval_data = self.generate_eval_data(test_questions, ground_truths)
        
        # í‰ê°€ ì‹¤í–‰
        results = evaluate(
            Dataset.from_dict(eval_data),
            metrics=[faithfulness, answer_relevancy]
        )
        
        # ê¸°ë¡
        record = {
            'date': datetime.now().isoformat(),
            'scores': results
        }
        self.history.append(record)
        
        # ì•Œë¦¼
        if results['faithfulness'] < self.threshold:
            self.send_alert(f"âš ï¸ Faithfulness dropped: {results['faithfulness']:.2f}")
        
        return results
    
    def send_alert(self, message):
        """ì•Œë¦¼ ì „ì†¡ (Slack, Email ë“±)"""
        print(f"ALERT: {message}")
        # ì‹¤ì œë¡œëŠ” Slack/Email ì „ì†¡

# ì‚¬ìš©
monitor = RAGMonitor(rag_system, alert_threshold=0.75)
monitor.daily_evaluation(test_questions, ground_truths)
```

---

## ğŸ“ ì •ë¦¬

### RAGAS ë„ì… ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] í‰ê°€ìš© í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ êµ¬ì¶• (ìµœì†Œ 50ê°œ)
- [ ] Ground truth ì •ë‹µ ì‘ì„±
- [ ] í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] CI/CDì— í‰ê°€ í†µí•©
- [ ] ì£¼ê¸°ì  ëª¨ë‹ˆí„°ë§ ì„¤ì •
- [ ] ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶•

### ê¶Œì¥ ì›Œí¬í”Œë¡œìš°

```python
# 1. ê°œë°œ ë‹¨ê³„
â†’ ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¹ ë¥´ê²Œ í‰ê°€
â†’ ê° ì§€í‘œë³„ ê°œì„  ì „ëµ ìˆ˜ë¦½

# 2. ë°°í¬ ì „
â†’ ëŒ€ê·œëª¨ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì² ì €íˆ í‰ê°€
â†’ ê¸°ì¤€ ì ìˆ˜ í†µê³¼ í™•ì¸

# 3. í”„ë¡œë•ì…˜
â†’ ì‹¤ì‹œê°„ ìƒ˜í”Œë§ í‰ê°€
â†’ ì£¼ê°„/ì›”ê°„ ì „ì²´ í‰ê°€
â†’ ì„±ëŠ¥ í‡´í™” ê°ì§€ ë° ì•Œë¦¼
```

RAGASë¥¼ í™œìš©í•˜ë©´ RAG ì‹œìŠ¤í…œì˜ í’ˆì§ˆì„ ê°ê´€ì ìœ¼ë¡œ ì¸¡ì •í•˜ê³  ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ“Š

---

ğŸ“š **ì°¸ê³  ìë£Œ**:
- [RAGAS ê³µì‹ ë¬¸ì„œ](https://docs.ragas.io/)
- [RAGAS GitHub](https://github.com/explodinggradients/ragas)
